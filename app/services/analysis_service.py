"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫.
–í–∫–ª—é—á–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Elasticsearch –∏ Prometheus.
"""

import uuid
import asyncio
import datetime
from typing import Optional, List, Dict, Any

import aiohttp
from elasticsearch import AsyncElasticsearch

from app.models.schemas import (
    LogAnalysisResult,
    MetricsAnomalyResult,
    RemediationPlan,
    ActionStatus,
    SeverityLevel
)
from app.services import telegram_service, ai_service, automation_service
from app.utils.logger import logger
from .system_service import save_plan_to_db, get_plan_from_db
from config.settings import settings


class DataCollector:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    
    def __init__(self):
        self.es_client: Optional[AsyncElasticsearch] = None
    
    async def get_es_client(self) -> AsyncElasticsearch:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Elasticsearch."""
        if self.es_client is None:
            self.es_client = AsyncElasticsearch(
                hosts=[f"http://{settings.elasticsearch_host}:{settings.elasticsearch_port}"],
                request_timeout=30
            )
        return self.es_client
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è."""
        if self.es_client:
            await self.es_client.close()
            self.es_client = None
    
    async def collect_logs_from_elasticsearch(
        self, 
        service_name: str, 
        time_window: str = "15m",
        log_level: str = "error"
    ) -> List[Dict[str, Any]]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –ª–æ–≥–∏ –∏–∑ Elasticsearch –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞.
        
        Args:
            service_name: –ò–º—è —Å–µ—Ä–≤–∏—Å–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ —á–∞—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞)
            time_window: –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "15m", "1h", "24h")
            log_level: –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (error, warning, info)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –ª–æ–≥–æ–≤
        """
        logger.info(f"–°–±–æ—Ä –ª–æ–≥–æ–≤ –∏–∑ Elasticsearch –¥–ª—è {service_name} –∑–∞ {time_window}...")
        
        try:
            es = await self.get_es_client()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
            index_patterns = [
                f"{service_name}-*",
                f"logs-{service_name}-*",
                f"filebeat-*"  # Fallback –Ω–∞ –æ–±—â–∏–π –∏–Ω–¥–µ–∫—Å filebeat
            ]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            query = {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "@timestamp": {
                                    "gte": f"now-{time_window}",
                                    "lt": "now"
                                }
                            }
                        }
                    ],
                    "should": [
                        {"match": {"log.level": log_level}},
                        {"match": {"level": log_level}},
                        {"match": {"severity": log_level}},
                        {"match_phrase": {"message": "error"}},
                        {"match_phrase": {"message": "ERROR"}},
                        {"match_phrase": {"message": "exception"}},
                        {"match_phrase": {"message": "failed"}}
                    ],
                    "minimum_should_match": 1
                }
            }
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–µ—Ä–≤–∏—Å, –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä
            if service_name != "all":
                query["bool"]["must"].append({
                    "bool": {
                        "should": [
                            {"match": {"service.name": service_name}},
                            {"match": {"kubernetes.labels.app": service_name}},
                            {"match": {"container.name": service_name}},
                            {"wildcard": {"source": f"*{service_name}*"}}
                        ],
                        "minimum_should_match": 1
                    }
                })
            
            logs = []
            for index_pattern in index_patterns:
                try:
                    response = await es.search(
                        index=index_pattern,
                        query=query,
                        size=100,
                        sort=[{"@timestamp": {"order": "desc"}}],
                        ignore_unavailable=True
                    )
                    
                    for hit in response["hits"]["hits"]:
                        source = hit["_source"]
                        logs.append({
                            "timestamp": source.get("@timestamp"),
                            "message": source.get("message", ""),
                            "level": source.get("log", {}).get("level") or source.get("level", "unknown"),
                            "service": source.get("service", {}).get("name") or service_name,
                            "source": source.get("source", ""),
                            "raw": source
                        })
                    
                    if logs:
                        break  # –ù–∞—à–ª–∏ –ª–æ–≥–∏, –≤—ã—Ö–æ–¥–∏–º
                        
                except Exception as e:
                    logger.debug(f"–ò–Ω–¥–µ–∫—Å {index_pattern} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –æ—à–∏–±–∫–∞: {e}")
                    continue
            
            logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(logs)} –∑–∞–ø–∏—Å–µ–π –ª–æ–≥–æ–≤ –¥–ª—è {service_name}")
            return logs
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –ª–æ–≥–æ–≤ –∏–∑ Elasticsearch: {e}")
            return []
    
    async def collect_metrics_from_prometheus(
        self, 
        service_name: str, 
        time_window: str = "15m"
    ) -> Dict[str, Any]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ Prometheus –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞.
        
        Args:
            service_name: –ò–º—è —Å–µ—Ä–≤–∏—Å–∞
            time_window: –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        logger.info(f"–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∏–∑ Prometheus –¥–ª—è {service_name} –∑–∞ {time_window}...")
        
        metrics = {
            "cpu_usage": None,
            "memory_usage": None,
            "error_rate": None,
            "request_latency": None,
            "availability": None
        }
        
        # –ó–∞–ø—Ä–æ—Å—ã PromQL –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        queries = {
            "cpu_usage": f'avg(rate(container_cpu_usage_seconds_total{{container="{service_name}"}}[{time_window}])) * 100',
            "memory_usage": f'avg(container_memory_usage_bytes{{container="{service_name}"}}) / 1024 / 1024',
            "error_rate": f'sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[{time_window}])) / sum(rate(http_requests_total{{service="{service_name}"}}[{time_window}])) * 100',
            "request_latency": f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{service="{service_name}"}}[{time_window}]))',
            "availability": f'avg_over_time(up{{job="{service_name}"}}[{time_window}]) * 100'
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                for metric_name, query in queries.items():
                    try:
                        url = f"{settings.prometheus_url}/api/v1/query"
                        params = {"query": query}
                        
                        async with session.get(url, params=params, timeout=10) as response:
                            if response.status == 200:
                                data = await response.json()
                                if data["status"] == "success" and data["data"]["result"]:
                                    value = float(data["data"]["result"][0]["value"][1])
                                    metrics[metric_name] = round(value, 2)
                                    
                    except Exception as e:
                        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫—É {metric_name}: {e}")
                        continue
            
            logger.info(f"–°–æ–±—Ä–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {service_name}: {metrics}")
            return metrics
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –º–µ—Ç—Ä–∏–∫ –∏–∑ Prometheus: {e}")
            return metrics


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
data_collector = DataCollector()


async def analyze_logs(service_name: str, time_window: str) -> LogAnalysisResult:
    """
    –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        service_name: –ò–º—è —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        time_window: –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ
        
    Returns:
        LogAnalysisResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    logger.info(f"–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –¥–ª—è {service_name} –∑–∞ {time_window}...")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏ –∏–∑ Elasticsearch
    logs = await data_collector.collect_logs_from_elasticsearch(service_name, time_window)
    
    if not logs:
        logger.info(f"–õ–æ–≥–∏ –¥–ª—è {service_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        return LogAnalysisResult(
            summary="–õ–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
            root_cause="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
            severity=SeverityLevel.LOW,
            relevant_logs=[]
        )
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ª–æ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ AI
    logs_text = "\n".join([
        f"[{log.get('timestamp', 'N/A')}] [{log.get('level', 'N/A')}] {log.get('message', '')}"
        for log in logs[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è AI
    ])
    
    # –í—ã–∑–æ–≤ AI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    analysis_result = await ai_service.analyze_logs_with_llm(logs_text)
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤: {analysis_result.summary}")
    
    return analysis_result


async def analyze_metrics(service_name: str, time_window: str) -> MetricsAnomalyResult:
    """
    –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        service_name: –ò–º—è —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        time_window: –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ
        
    Returns:
        MetricsAnomalyResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    logger.info(f"–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –¥–ª—è {service_name} –∑–∞ {time_window}...")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ Prometheus
    metrics = await data_collector.collect_metrics_from_prometheus(service_name, time_window)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    anomaly_score = 0.0
    anomaly_description = []
    primary_metric = "system"
    primary_value = 0.0
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CPU
    if metrics["cpu_usage"] is not None:
        if metrics["cpu_usage"] > 90:
            anomaly_score = max(anomaly_score, 0.95)
            anomaly_description.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU: {metrics['cpu_usage']}%")
            primary_metric = "cpu_usage"
            primary_value = metrics["cpu_usage"]
        elif metrics["cpu_usage"] > 80:
            anomaly_score = max(anomaly_score, 0.8)
            anomaly_description.append(f"–í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU: {metrics['cpu_usage']}%")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
    if metrics["memory_usage"] is not None:
        if metrics["memory_usage"] > 90:
            anomaly_score = max(anomaly_score, 0.95)
            anomaly_description.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {metrics['memory_usage']}%")
            if primary_metric == "system":
                primary_metric = "memory_usage"
                primary_value = metrics["memory_usage"]
        elif metrics["memory_usage"] > 80:
            anomaly_score = max(anomaly_score, 0.75)
            anomaly_description.append(f"–í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {metrics['memory_usage']}%")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ error rate
    if metrics["error_rate"] is not None:
        if metrics["error_rate"] > 10:
            anomaly_score = max(anomaly_score, 0.98)
            anomaly_description.append(f"–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –æ—à–∏–±–æ–∫: {metrics['error_rate']}%")
            primary_metric = "error_rate"
            primary_value = metrics["error_rate"]
        elif metrics["error_rate"] > 5:
            anomaly_score = max(anomaly_score, 0.85)
            anomaly_description.append(f"–ü–æ–≤—ã—à–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –æ—à–∏–±–æ–∫: {metrics['error_rate']}%")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
    if metrics["availability"] is not None and metrics["availability"] < 99:
        anomaly_score = max(anomaly_score, 0.9)
        anomaly_description.append(f"–ù–∏–∑–∫–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: {metrics['availability']}%")
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∏–∑–∫–∏–π anomaly_score
    if all(v is None for v in metrics.values()):
        logger.warning(f"–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {service_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        anomaly_score = 0.1
        anomaly_description = ["–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"]
    
    description = "; ".join(anomaly_description) if anomaly_description else "–ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
    
    return MetricsAnomalyResult(
        metric_name=primary_metric,
        current_value=primary_value,
        expected_range=(0, 80),
        anomaly_score=anomaly_score,
        description=description,
        timestamp=datetime.datetime.now()
    )


async def generate_remediation_plan(
    log_result: LogAnalysisResult,
    metrics_result: MetricsAnomalyResult
) -> RemediationPlan:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞.
    
    Args:
        log_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤
        metrics_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç—Ä–∏–∫
        
    Returns:
        RemediationPlan —Å –ø–ª–∞–Ω–æ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    """
    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è...")
    
    context = f"""
–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –≤ IT-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ:

## –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫:
- –ú–µ—Ç—Ä–∏–∫–∞: {metrics_result.metric_name}
- –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {metrics_result.current_value}
- –û–∂–∏–¥–∞–µ–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: {metrics_result.expected_range}
- –û–ø–∏—Å–∞–Ω–∏–µ: {metrics_result.description}

## –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤:
- –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {log_result.summary}
- –ü–µ—Ä–≤–æ–ø—Ä–∏—á–∏–Ω–∞: {log_result.root_cause}
- –£—Ä–æ–≤–µ–Ω—å –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏: {log_result.severity}
- –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ª–æ–≥–∏:
{chr(10).join(log_result.relevant_logs[:5]) if log_result.relevant_logs else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}
"""
    
    playbook_yaml = await ai_service.generate_remediation_plan(context)
    
    plan = RemediationPlan(
        plan_id=str(uuid.uuid4()),
        title=f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {log_result.summary[:50]}",
        description=context,
        severity=log_result.severity,
        playbook_yaml=playbook_yaml,
        estimated_duration=60,
        created_at=datetime.datetime.now()
    )
    
    await save_plan_to_db(plan)
    logger.info(f"–°–æ–∑–¥–∞–Ω –ø–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {plan.plan_id}")
    return plan


async def trigger_full_analysis(service_name: str, time_window: str = "15m"):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞.
    
    Args:
        service_name: –ò–º—è —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        time_window: –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    """
    try:
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
        metrics_anomaly = await analyze_metrics(service_name, time_window)
        
        if metrics_anomaly.anomaly_score > 0.7:
            await telegram_service.send_message(
                f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö *{service_name}*:\n{metrics_anomaly.description}"
            )
            
            # –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
            log_analysis = await analyze_logs(service_name, time_window)
            
            # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞
            if log_analysis.severity in [SeverityLevel.HIGH, SeverityLevel.CRITICAL]:
                remediation_plan = await generate_remediation_plan(log_analysis, metrics_anomaly)
                
                # –®–∞–≥ 4: –û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
                await telegram_service.send_approval_request(remediation_plan)
            else:
                await telegram_service.send_message(
                    f"‚ÑπÔ∏è –ü—Ä–æ–±–ª–µ–º–∞ –≤ *{service_name}* –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞.\n"
                    f"–£—Ä–æ–≤–µ–Ω—å: {log_analysis.severity.value}\n"
                    f"–û–ø–∏—Å–∞–Ω–∏–µ: {log_analysis.summary}"
                )
        elif metrics_anomaly.anomaly_score > 0.5:
            # –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å - —Ç–æ–ª—å–∫–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
            await telegram_service.send_message(
                f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö *{service_name}*:\n{metrics_anomaly.description}\n\n"
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Ç—É–∞—Ü–∏–∏."
            )
        else:
            await telegram_service.send_message(
                f"‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–ª—è *{service_name}* –∑–∞–≤–µ—Ä—à–µ–Ω.\n–ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
            )
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ —Å–µ—Ä–≤–∏—Å–∞ {service_name}: {e}", exc_info=True)
        await telegram_service.send_message(
            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–µ—Ä–≤–∏—Å–∞ *{service_name}*:\n`{str(e)[:200]}`"
        )


async def process_approval(plan_id: str, approved: bool, reason: str = None) -> str:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞.
    
    Args:
        plan_id: ID –ø–ª–∞–Ω–∞
        approved: –£—Ç–≤–µ—Ä–∂–¥–µ–Ω –ª–∏ –ø–ª–∞–Ω
        reason: –ü—Ä–∏—á–∏–Ω–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –°–æ–æ–±—â–µ–Ω–∏–µ –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
    """
    plan = await get_plan_from_db(plan_id)
    
    if plan.status != ActionStatus.PENDING:
        return f"–î–µ–π—Å—Ç–≤–∏–µ –ø–æ –ø–ª–∞–Ω—É {plan_id} —É–∂–µ –±—ã–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ."

    if approved:
        plan.status = ActionStatus.APPROVED
        plan.approved_at = datetime.datetime.now()
        await save_plan_to_db(plan)
        
        await telegram_service.send_message(
            f"üöÄ –ü–ª–∞–Ω *{plan.title}* —É—Ç–≤–µ—Ä–∂–¥–µ–Ω.\n–ù–∞—á–∏–Ω–∞—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ..."
        )
        
        # –ó–∞–ø—É—Å–∫ Ansible –ø–ª–µ–π–±—É–∫–∞
        await automation_service.run_playbook_async(plan)
        return f"–ü–ª–∞–Ω {plan_id} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω –∏ –ø–µ—Ä–µ–¥–∞–Ω –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ."
    else:
        plan.status = ActionStatus.REJECTED
        await save_plan_to_db(plan)
        
        rejection_message = f"‚ùå –ü–ª–∞–Ω *{plan.title}* –æ—Ç–∫–ª–æ–Ω–µ–Ω."
        if reason:
            rejection_message += f"\n–ü—Ä–∏—á–∏–Ω–∞: {reason}"
        await telegram_service.send_message(rejection_message)
        
        return f"–ü–ª–∞–Ω {plan_id} –æ—Ç–∫–ª–æ–Ω–µ–Ω."
