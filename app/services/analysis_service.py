"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫.

Features:
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Elasticsearch –∏ Prometheus
- Circuit Breaker –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç —Å–±–æ–µ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
- Alertmanager webhooks –¥–ª—è push-–º–æ–¥–µ–ª–∏
"""

import uuid
import asyncio
import datetime
from typing import Optional, List, Dict, Any

import aiohttp
from elasticsearch import AsyncElasticsearch

from app.models.schemas import (
    LogAnalysisResult,
    MetricsAnomalyResult,
    RemediationPlan,
    ActionStatus,
    SeverityLevel
)
from app.services import telegram_service, ai_service, automation_service
from app.utils.logger import logger
from app.utils.circuit_breaker import (
    CircuitBreaker, 
    CircuitBreakerConfig,
    CircuitBreakerOpenError,
    elasticsearch_breaker,
    prometheus_breaker
)
from .system_service import save_plan_to_db, get_plan_from_db
from config.settings import settings


class DataCollector:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    
    def __init__(self):
        self.es_client: Optional[AsyncElasticsearch] = None
        self._http_session: Optional[aiohttp.ClientSession] = None
    
    async def get_es_client(self) -> AsyncElasticsearch:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Elasticsearch."""
        if self.es_client is None:
            self.es_client = AsyncElasticsearch(
                hosts=[f"http://{settings.elasticsearch_host}:{settings.elasticsearch_port}"],
                request_timeout=30
            )
        return self.es_client
    
    async def get_http_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç HTTP —Å–µ—Å—Å–∏—é –¥–ª—è Prometheus."""
        if self._http_session is None or self._http_session.closed:
            self._http_session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10)
            )
        return self._http_session
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è."""
        if self.es_client:
            await self.es_client.close()
            self.es_client = None
        if self._http_session and not self._http_session.closed:
            await self._http_session.close()
            self._http_session = None
    
    async def collect_logs_from_elasticsearch(
        self, 
        service_name: str, 
        time_window: str = "15m",
        log_level: str = "error"
    ) -> List[Dict[str, Any]]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –ª–æ–≥–∏ –∏–∑ Elasticsearch —Å circuit breaker –∑–∞—â–∏—Ç–æ–π.
        """
        logger.info(f"–°–±–æ—Ä –ª–æ–≥–æ–≤ –∏–∑ Elasticsearch –¥–ª—è {service_name} –∑–∞ {time_window}...")
        
        # Check circuit breaker
        if elasticsearch_breaker.is_open:
            logger.warning("Elasticsearch circuit breaker is OPEN, returning empty logs")
            return []
        
        try:
            return await elasticsearch_breaker.call(
                self._collect_logs_internal,
                service_name, time_window, log_level
            )
        except CircuitBreakerOpenError:
            logger.warning("Elasticsearch circuit breaker triggered")
            return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –ª–æ–≥–æ–≤: {e}")
            return []
    
    async def _collect_logs_internal(
        self,
        service_name: str,
        time_window: str,
        log_level: str
    ) -> List[Dict[str, Any]]:
        """Internal method for log collection."""
        es = await self.get_es_client()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å
        index_patterns = [
            f"{service_name}-*",
            f"logs-{service_name}-*",
            f"filebeat-*"
        ]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query = {
            "bool": {
                "must": [
                    {
                        "range": {
                            "@timestamp": {
                                "gte": f"now-{time_window}",
                                "lt": "now"
                            }
                        }
                    }
                ],
                "should": [
                    {"match": {"log.level": log_level}},
                    {"match": {"level": log_level}},
                    {"match": {"severity": log_level}},
                    {"match_phrase": {"message": "error"}},
                    {"match_phrase": {"message": "ERROR"}},
                    {"match_phrase": {"message": "exception"}},
                    {"match_phrase": {"message": "failed"}}
                ],
                "minimum_should_match": 1
            }
        }
        
        if service_name != "all":
            query["bool"]["must"].append({
                "bool": {
                    "should": [
                        {"match": {"service.name": service_name}},
                        {"match": {"kubernetes.labels.app": service_name}},
                        {"match": {"container.name": service_name}},
                        {"wildcard": {"source": f"*{service_name}*"}}
                    ],
                    "minimum_should_match": 1
                }
            })
        
        logs = []
        for index_pattern in index_patterns:
            try:
                response = await es.search(
                    index=index_pattern,
                    query=query,
                    size=100,
                    sort=[{"@timestamp": {"order": "desc"}}],
                    ignore_unavailable=True
                )
                
                for hit in response["hits"]["hits"]:
                    source = hit["_source"]
                    logs.append({
                        "timestamp": source.get("@timestamp"),
                        "message": source.get("message", ""),
                        "level": source.get("log", {}).get("level") or source.get("level", "unknown"),
                        "service": source.get("service", {}).get("name") or service_name,
                        "source": source.get("source", ""),
                        "raw": source
                    })
                
                if logs:
                    break
                    
            except Exception as e:
                logger.debug(f"–ò–Ω–¥–µ–∫—Å {index_pattern} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –æ—à–∏–±–∫–∞: {e}")
                continue
        
        logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(logs)} –∑–∞–ø–∏—Å–µ–π –ª–æ–≥–æ–≤ –¥–ª—è {service_name}")
        return logs
    
    async def collect_metrics_from_prometheus(
        self, 
        service_name: str, 
        time_window: str = "15m"
    ) -> Dict[str, Any]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ Prometheus —Å circuit breaker –∑–∞—â–∏—Ç–æ–π.
        """
        logger.info(f"–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∏–∑ Prometheus –¥–ª—è {service_name} –∑–∞ {time_window}...")
        
        metrics = {
            "cpu_usage": None,
            "memory_usage": None,
            "error_rate": None,
            "request_latency": None,
            "availability": None
        }
        
        # Check circuit breaker
        if prometheus_breaker.is_open:
            logger.warning("Prometheus circuit breaker is OPEN, returning empty metrics")
            return metrics
        
        try:
            return await prometheus_breaker.call(
                self._collect_metrics_internal,
                service_name, time_window
            )
        except CircuitBreakerOpenError:
            logger.warning("Prometheus circuit breaker triggered")
            return metrics
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –º–µ—Ç—Ä–∏–∫: {e}")
            return metrics
    
    async def _collect_metrics_internal(
        self,
        service_name: str,
        time_window: str
    ) -> Dict[str, Any]:
        """Internal method for metrics collection."""
        metrics = {
            "cpu_usage": None,
            "memory_usage": None,
            "error_rate": None,
            "request_latency": None,
            "availability": None
        }
        
        queries = {
            "cpu_usage": f'avg(rate(container_cpu_usage_seconds_total{{container="{service_name}"}}[{time_window}])) * 100',
            "memory_usage": f'avg(container_memory_usage_bytes{{container="{service_name}"}}) / 1024 / 1024',
            "error_rate": f'sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[{time_window}])) / sum(rate(http_requests_total{{service="{service_name}"}}[{time_window}])) * 100',
            "request_latency": f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{service="{service_name}"}}[{time_window}]))',
            "availability": f'avg_over_time(up{{job="{service_name}"}}[{time_window}]) * 100'
        }
        
        session = await self.get_http_session()
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
        async def fetch_metric(metric_name: str, query: str) -> tuple:
            try:
                url = f"{settings.prometheus_url}/api/v1/query"
                params = {"query": query}
                
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data["status"] == "success" and data["data"]["result"]:
                            value = float(data["data"]["result"][0]["value"][1])
                            return (metric_name, round(value, 2))
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫—É {metric_name}: {e}")
            return (metric_name, None)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [fetch_metric(name, query) for name, query in queries.items()]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, tuple):
                metric_name, value = result
                metrics[metric_name] = value
        
        logger.info(f"–°–æ–±—Ä–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {service_name}: {metrics}")
        return metrics
    
    async def collect_all_data_parallel(
        self,
        service_name: str,
        time_window: str = "15m"
    ) -> Dict[str, Any]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö (–ª–æ–≥–∏ + –º–µ—Ç—Ä–∏–∫–∏).
        
        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏ 'logs' –∏ 'metrics'
        """
        logger.info(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {service_name}...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–±–æ—Ä –ª–æ–≥–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        logs_task = asyncio.create_task(
            self.collect_logs_from_elasticsearch(service_name, time_window)
        )
        metrics_task = asyncio.create_task(
            self.collect_metrics_from_prometheus(service_name, time_window)
        )
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±–æ–∏—Ö –∑–∞–¥–∞—á
        logs, metrics = await asyncio.gather(logs_task, metrics_task, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        if isinstance(logs, Exception):
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –ª–æ–≥–æ–≤: {logs}")
            logs = []
        if isinstance(metrics, Exception):
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫: {metrics}")
            metrics = {}
        
        return {
            "logs": logs,
            "metrics": metrics,
            "collected_at": datetime.datetime.now().isoformat()
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
data_collector = DataCollector()


# ==================== Alertmanager Webhook Handler ====================

async def handle_alertmanager_webhook(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ webhook –æ—Ç Alertmanager (push-–º–æ–¥–µ–ª—å).
    
    Args:
        payload: JSON payload –æ—Ç Alertmanager
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    logger.info(f"–ü–æ–ª—É—á–µ–Ω webhook –æ—Ç Alertmanager: {payload.get('status')}")
    
    alerts = payload.get("alerts", [])
    processed = []
    
    for alert in alerts:
        alert_name = alert.get("labels", {}).get("alertname", "unknown")
        service = alert.get("labels", {}).get("service") or alert.get("labels", {}).get("job", "unknown")
        status = alert.get("status", "unknown")
        severity = alert.get("labels", {}).get("severity", "warning")
        description = alert.get("annotations", {}).get("description", "")
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–ª–µ—Ä—Ç–∞: {alert_name} –¥–ª—è {service} ({status})")
        
        if status == "firing":
            # –ê–ª–µ—Ä—Ç –∞–∫—Ç–∏–≤–µ–Ω - –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º severity
                severity_map = {
                    "critical": SeverityLevel.CRITICAL,
                    "high": SeverityLevel.HIGH,
                    "warning": SeverityLevel.MEDIUM,
                    "info": SeverityLevel.LOW
                }
                sev = severity_map.get(severity.lower(), SeverityLevel.MEDIUM)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
                await telegram_service.send_message(
                    f"üö® *Alertmanager*: {alert_name}\n"
                    f"–°–µ—Ä–≤–∏—Å: {service}\n"
                    f"Severity: {severity}\n"
                    f"–û–ø–∏—Å–∞–Ω–∏–µ: {description[:200]}"
                )
                
                # –î–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                if sev in [SeverityLevel.CRITICAL, SeverityLevel.HIGH]:
                    asyncio.create_task(trigger_full_analysis(service, "15m"))
                
                processed.append({
                    "alert": alert_name,
                    "service": service,
                    "action": "analysis_triggered" if sev in [SeverityLevel.CRITICAL, SeverityLevel.HIGH] else "notified"
                })
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–ª–µ—Ä—Ç–∞ {alert_name}: {e}")
                processed.append({
                    "alert": alert_name,
                    "error": str(e)
                })
        
        elif status == "resolved":
            # –ê–ª–µ—Ä—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω
            await telegram_service.send_message(
                f"‚úÖ *Resolved*: {alert_name}\n"
                f"–°–µ—Ä–≤–∏—Å: {service}"
            )
            processed.append({
                "alert": alert_name,
                "service": service,
                "action": "resolved_notified"
            })
    
    return {
        "status": "processed",
        "alerts_count": len(alerts),
        "processed": processed
    }


# ==================== Analysis Functions ====================

async def analyze_logs(service_name: str, time_window: str) -> LogAnalysisResult:
    """
    –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    logger.info(f"–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –¥–ª—è {service_name} –∑–∞ {time_window}...")
    
    logs = await data_collector.collect_logs_from_elasticsearch(service_name, time_window)
    
    if not logs:
        logger.info(f"–õ–æ–≥–∏ –¥–ª—è {service_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return LogAnalysisResult(
            summary="–õ–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
            root_cause="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
            severity=SeverityLevel.LOW,
            relevant_logs=[]
        )
    
    logs_text = "\n".join([
        f"[{log.get('timestamp', 'N/A')}] [{log.get('level', 'N/A')}] {log.get('message', '')}"
        for log in logs[:50]
    ])
    
    analysis_result = await ai_service.analyze_logs_with_llm(logs_text)
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤: {analysis_result.summary}")
    
    return analysis_result


async def analyze_metrics(service_name: str, time_window: str) -> MetricsAnomalyResult:
    """
    –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    logger.info(f"–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –¥–ª—è {service_name} –∑–∞ {time_window}...")
    
    metrics = await data_collector.collect_metrics_from_prometheus(service_name, time_window)
    
    anomaly_score = 0.0
    anomaly_description = []
    primary_metric = "system"
    primary_value = 0.0
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CPU
    if metrics["cpu_usage"] is not None:
        if metrics["cpu_usage"] > 90:
            anomaly_score = max(anomaly_score, 0.95)
            anomaly_description.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU: {metrics['cpu_usage']}%")
            primary_metric = "cpu_usage"
            primary_value = metrics["cpu_usage"]
        elif metrics["cpu_usage"] > 80:
            anomaly_score = max(anomaly_score, 0.8)
            anomaly_description.append(f"–í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU: {metrics['cpu_usage']}%")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
    if metrics["memory_usage"] is not None:
        if metrics["memory_usage"] > 90:
            anomaly_score = max(anomaly_score, 0.95)
            anomaly_description.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {metrics['memory_usage']}%")
            if primary_metric == "system":
                primary_metric = "memory_usage"
                primary_value = metrics["memory_usage"]
        elif metrics["memory_usage"] > 80:
            anomaly_score = max(anomaly_score, 0.75)
            anomaly_description.append(f"–í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {metrics['memory_usage']}%")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ error rate
    if metrics["error_rate"] is not None:
        if metrics["error_rate"] > 10:
            anomaly_score = max(anomaly_score, 0.98)
            anomaly_description.append(f"–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –æ—à–∏–±–æ–∫: {metrics['error_rate']}%")
            primary_metric = "error_rate"
            primary_value = metrics["error_rate"]
        elif metrics["error_rate"] > 5:
            anomaly_score = max(anomaly_score, 0.85)
            anomaly_description.append(f"–ü–æ–≤—ã—à–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –æ—à–∏–±–æ–∫: {metrics['error_rate']}%")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
    if metrics["availability"] is not None and metrics["availability"] < 99:
        anomaly_score = max(anomaly_score, 0.9)
        anomaly_description.append(f"–ù–∏–∑–∫–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: {metrics['availability']}%")
    
    if all(v is None for v in metrics.values()):
        logger.warning(f"–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {service_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        anomaly_score = 0.1
        anomaly_description = ["–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"]
    
    description = "; ".join(anomaly_description) if anomaly_description else "–ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
    
    return MetricsAnomalyResult(
        metric_name=primary_metric,
        current_value=primary_value,
        expected_range=(0, 80),
        anomaly_score=anomaly_score,
        description=description,
        timestamp=datetime.datetime.now()
    )


async def generate_remediation_plan(
    log_result: LogAnalysisResult,
    metrics_result: MetricsAnomalyResult
) -> RemediationPlan:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞.
    """
    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è...")
    
    context = f"""
–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –≤ IT-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ:

## –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫:
- –ú–µ—Ç—Ä–∏–∫–∞: {metrics_result.metric_name}
- –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {metrics_result.current_value}
- –û–∂–∏–¥–∞–µ–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: {metrics_result.expected_range}
- –û–ø–∏—Å–∞–Ω–∏–µ: {metrics_result.description}

## –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤:
- –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {log_result.summary}
- –ü–µ—Ä–≤–æ–ø—Ä–∏—á–∏–Ω–∞: {log_result.root_cause}
- –£—Ä–æ–≤–µ–Ω—å –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏: {log_result.severity}
- –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ª–æ–≥–∏:
{chr(10).join(log_result.relevant_logs[:5]) if log_result.relevant_logs else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}
"""
    
    playbook_yaml = await ai_service.generate_remediation_plan(context)
    
    plan = RemediationPlan(
        plan_id=str(uuid.uuid4()),
        title=f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {log_result.summary[:50]}",
        description=context,
        severity=log_result.severity,
        playbook_yaml=playbook_yaml,
        estimated_duration=60,
        created_at=datetime.datetime.now()
    )
    
    await save_plan_to_db(plan)
    logger.info(f"–°–æ–∑–¥–∞–Ω –ø–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {plan.plan_id}")
    return plan


async def trigger_full_analysis(service_name: str, time_window: str = "15m"):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º —Å–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö.
    """
    try:
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        all_data = await data_collector.collect_all_data_parallel(service_name, time_window)
        
        # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
        metrics_anomaly = await analyze_metrics(service_name, time_window)
        
        if metrics_anomaly.anomaly_score > 0.7:
            await telegram_service.send_message(
                f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö *{service_name}*:\n{metrics_anomaly.description}"
            )
            
            # –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
            log_analysis = await analyze_logs(service_name, time_window)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞
            if log_analysis.severity in [SeverityLevel.HIGH, SeverityLevel.CRITICAL]:
                remediation_plan = await generate_remediation_plan(log_analysis, metrics_anomaly)
                await telegram_service.send_approval_request(remediation_plan)
            else:
                await telegram_service.send_message(
                    f"‚ÑπÔ∏è –ü—Ä–æ–±–ª–µ–º–∞ –≤ *{service_name}* –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞.\n"
                    f"–£—Ä–æ–≤–µ–Ω—å: {log_analysis.severity.value}\n"
                    f"–û–ø–∏—Å–∞–Ω–∏–µ: {log_analysis.summary}"
                )
        elif metrics_anomaly.anomaly_score > 0.5:
            await telegram_service.send_message(
                f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö *{service_name}*:\n{metrics_anomaly.description}\n\n"
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Ç—É–∞—Ü–∏–∏."
            )
        else:
            await telegram_service.send_message(
                f"‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–ª—è *{service_name}* –∑–∞–≤–µ—Ä—à–µ–Ω.\n–ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
            )
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ —Å–µ—Ä–≤–∏—Å–∞ {service_name}: {e}", exc_info=True)
        await telegram_service.send_message(
            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–µ—Ä–≤–∏—Å–∞ *{service_name}*:\n`{str(e)[:200]}`"
        )


async def process_approval(plan_id: str, approved: bool, reason: str = None) -> str:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞.
    """
    plan = await get_plan_from_db(plan_id)
    
    if plan.status != ActionStatus.PENDING:
        return f"–î–µ–π—Å—Ç–≤–∏–µ –ø–æ –ø–ª–∞–Ω—É {plan_id} —É–∂–µ –±—ã–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ."

    if approved:
        plan.status = ActionStatus.APPROVED
        plan.approved_at = datetime.datetime.now()
        await save_plan_to_db(plan)
        
        await telegram_service.send_message(
            f"üöÄ –ü–ª–∞–Ω *{plan.title}* —É—Ç–≤–µ—Ä–∂–¥–µ–Ω.\n–ù–∞—á–∏–Ω–∞—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ..."
        )
        
        await automation_service.run_playbook_async(plan)
        return f"–ü–ª–∞–Ω {plan_id} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω –∏ –ø–µ—Ä–µ–¥–∞–Ω –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ."
    else:
        plan.status = ActionStatus.REJECTED
        await save_plan_to_db(plan)
        
        rejection_message = f"‚ùå –ü–ª–∞–Ω *{plan.title}* –æ—Ç–∫–ª–æ–Ω–µ–Ω."
        if reason:
            rejection_message += f"\n–ü—Ä–∏—á–∏–Ω–∞: {reason}"
        await telegram_service.send_message(rejection_message)
        
        return f"–ü–ª–∞–Ω {plan_id} –æ—Ç–∫–ª–æ–Ω–µ–Ω."


# ==================== Health Check ====================

async def get_data_sources_status() -> Dict[str, Any]:
    """Get status of all data sources."""
    return {
        "elasticsearch": elasticsearch_breaker.get_status(),
        "prometheus": prometheus_breaker.get_status(),
    }
